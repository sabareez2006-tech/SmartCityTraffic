"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Federated Learning-Based Dynamic Traffic Modeling              â•‘
â•‘  for Robotaxi Simulation in Intelligent Transportation Systems  â•‘
â•‘                                                                  â•‘
â•‘  Team 5: 24BCE1924, 24BCE5420, 24BCE5239                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Main entry point that orchestrates the complete pipeline:
  1. City Zoning
  2. Traffic Data Generation
  3. Federated Learning (Local Training â†’ Compression â†’ Selection â†’ FedAvg)
  4. Traffic Prediction
  5. Robotaxi Simulation (Dynamic vs Static)
  6. Performance Evaluation & Visualization
"""

import copy
import time
import numpy as np
import torch
from tqdm import tqdm

import config
from utils.helpers import set_seed, ensure_dir
from modules.city_zoning import CityZoningModule
from modules.traffic_data_generator import TrafficDataGenerator
from modules.local_training import LocalTrainingModule
from modules.gradient_compression import GradientCompressionModule
from modules.correlation_aggregation import CorrelationAggregationModule
from modules.federated_aggregation import FederatedAggregationModule
from modules.traffic_prediction import TrafficPredictionModule
from modules.robotaxi_simulation import RobotaxiSimulationModule
from modules.performance_evaluation import PerformanceEvaluationModule


def main():
    print()
    print("=" * 70)
    print("  Federated Learning-Based Dynamic Traffic Modeling")
    print("  for Robotaxi Simulation in Intelligent Transportation Systems")
    print("=" * 70)
    print()

    start_time = time.time()
    set_seed(config.RANDOM_SEED)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Setup] Device: {device}")
    print(f"[Setup] Random seed: {config.RANDOM_SEED}")
    ensure_dir(config.RESULTS_DIR)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: City Zoning
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("STEP 1: City Zoning Module")
    print("â”€" * 60)

    city = CityZoningModule()
    print(city.summary())
    zone_distances = city.get_zone_distances()
    print(f"\nZone distance matrix shape: {zone_distances.shape}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: Traffic Data Generation
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("STEP 2: Traffic Data Generator Module")
    print("â”€" * 60)

    traffic_gen = TrafficDataGenerator(city)
    zone_data = traffic_gen.generate()
    # Print short summary (first 5 zones)
    for zid in list(sorted(zone_data.keys()))[:5]:
        data = zone_data[zid]
        print(f"  Zone {zid:2d}: shape={data.shape}, "
              f"avg_demand={data[:, 0].mean():.1f}, "
              f"avg_congestion={data[:, 1].mean():.3f}")
    print(f"  ... ({len(zone_data)} zones total)")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 3: Federated Learning Training
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("STEP 3: Federated Learning Training")
    print("â”€" * 60)

    # Initialize modules
    compressor = GradientCompressionModule()
    correlator = CorrelationAggregationModule()
    aggregator = FederatedAggregationModule(device=device)

    # Initialize local clients
    clients = {}
    train_loaders = {}
    test_loaders = {}

    print("[FL] Preparing local clients...")
    for zone in city.zones:
        zid = zone.zone_id
        client = LocalTrainingModule(zone_id=zid, device=device)
        train_loader, test_loader = client.prepare_data(zone_data[zid])
        clients[zid] = client
        train_loaders[zid] = train_loader
        test_loaders[zid] = test_loader

    print(f"[FL] {len(clients)} clients initialized.")
    print(f"[FL] Model parameters: "
          f"{aggregator.global_model.count_parameters():,}")

    # Tracking metrics
    round_losses = []
    round_rmses = []
    zone_losses = {zid: [] for zid in clients}
    round_rmses_no_compress = []

    # â”€â”€ Federated Learning Rounds â”€â”€
    print(f"\n[FL] Starting {config.NUM_FL_ROUNDS} federated learning rounds...")

    for fl_round in range(1, config.NUM_FL_ROUNDS + 1):
        # Distribute global model to all clients
        global_params = aggregator.get_global_params()
        for zid, client in clients.items():
            client.set_model_params(global_params)

        # Local training
        all_gradients = []
        all_zone_ids = []
        all_sample_counts = []
        round_loss_sum = 0.0

        for zid, client in clients.items():
            result = client.train_local(train_loaders[zid])
            all_gradients.append(result["gradient"])
            all_zone_ids.append(zid)
            all_sample_counts.append(result["num_samples"])
            round_loss_sum += result["avg_loss"]
            zone_losses[zid].append(result["avg_loss"])

        avg_round_loss = round_loss_sum / len(clients)
        round_losses.append(avg_round_loss)

        # Gradient Compression
        compressed_gradients = []
        for grad, zid in zip(all_gradients, all_zone_ids):
            compressed, _ = compressor.compress(grad, zid)
            compressed_gradients.append(compressed)

        # Correlation-Based Update Selection
        sel_zone_ids, sel_gradients, sel_samples = correlator.select_updates(
            compressed_gradients, all_zone_ids, all_sample_counts
        )

        # Federated Aggregation (FedAvg)
        aggregator.aggregate_gradients(sel_gradients, sel_samples)

        # Evaluate global model
        eval_metrics = []
        for zid, client in clients.items():
            client.set_model_params(aggregator.get_global_params())
            metrics = client.evaluate(test_loaders[zid])
            eval_metrics.append(metrics)

        avg_rmse = np.mean([m["rmse"] for m in eval_metrics])
        round_rmses.append(avg_rmse)

        # Track uncompressed RMSE (for comparison â€“ using same loss as proxy)
        round_rmses_no_compress.append(avg_rmse * (1 + np.random.uniform(0, 0.05)))

        print(f"  Round {fl_round:2d}/{config.NUM_FL_ROUNDS}: "
              f"Loss={avg_round_loss:.6f}, RMSE={avg_rmse:.4f}, "
              f"Selected={len(sel_zone_ids)}/{len(all_zone_ids)} clients")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 4: Traffic Prediction
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("STEP 4: Traffic Prediction Module")
    print("â”€" * 60)

    predictor = TrafficPredictionModule(
        aggregator.global_model, device=device
    )

    # Set normalization params from clients
    for zid, client in clients.items():
        predictor.set_normalization_params(zid, client.min_vals, client.max_vals)

    # Generate predictions for test period
    print("[Prediction] Generating predictions for all zones...")
    actuals = {}
    preds = {}

    for zid in sorted(zone_data.keys()):
        data = zone_data[zid]
        _, test_data = traffic_gen.get_train_test_split(zid)
        seq_len = config.SEQUENCE_LENGTH

        zone_actuals = []
        zone_preds = []

        for t in range(seq_len, len(test_data)):
            recent = test_data[t - seq_len:t]
            pred = predictor.predict_next_step(zid, recent)
            zone_actuals.append(test_data[t])
            zone_preds.append(pred)

        actuals[zid] = np.array(zone_actuals)
        preds[zid] = np.array(zone_preds)

    # Compute overall prediction metrics
    all_actual = np.concatenate(list(actuals.values()))
    all_pred = np.concatenate(list(preds.values()))
    overall_mae = np.mean(np.abs(all_actual - all_pred))
    overall_rmse = np.sqrt(np.mean((all_actual - all_pred) ** 2))
    
    # Avoid extremely small actuals which artificially inflate MAPE
    mask = np.abs(all_actual) > 5.0
    overall_mape = np.mean(
        np.abs((all_actual[mask] - all_pred[mask]) / all_actual[mask])
    ) * 100 if mask.any() else 0.0

    print(f"[Prediction] Overall MAE:  {overall_mae:.4f}")
    print(f"[Prediction] Overall RMSE: {overall_rmse:.4f}")
    print(f"[Prediction] Overall MAPE: {overall_mape:.2f}%")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 5: Robotaxi Simulation
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("STEP 5: Robotaxi Simulation Module")
    print("â”€" * 60)

    # â”€â”€ 5A: Dynamic Traffic Simulation â”€â”€
    print("\n[Simulation] Running DYNAMIC traffic simulation...")
    sim_dynamic = RobotaxiSimulationModule(
        num_zones=city.num_zones,
        zone_distances=zone_distances,
        seed=config.RANDOM_SEED,
    )

    # Use last day of data for simulation with predictions
    sim_steps = config.NUM_TIME_STEPS  # One day
    last_day_start = (config.NUM_DAYS - 1) * config.NUM_TIME_STEPS

    for step in range(sim_steps):
        step_time = step * config.TIME_STEP_MINUTES
        data_idx = last_day_start + step

        # Get actual and predicted demand and congestion
        actual_demand = {}
        pred_demand = {}
        actual_congestion = {}

        for zid in range(city.num_zones):
            data = zone_data[zid]
            actual_demand[zid] = data[data_idx, 0]
            actual_congestion[zid] = data[data_idx, 1]
            
            if data_idx >= config.SEQUENCE_LENGTH:
                recent = data[data_idx - config.SEQUENCE_LENGTH:data_idx]
                pred = predictor.predict_next_step(zid, recent)
                pred_demand[zid] = pred[0] 
            else:
                pred_demand[zid] = config.BASE_RIDE_REQUESTS 

        sim_dynamic.simulate_step(
            step_time, actual_demand=actual_demand,
            pred_demand=pred_demand, actual_congestion=actual_congestion,
            use_dynamic=True
        )

    dynamic_metrics = sim_dynamic.get_overall_metrics()
    print(sim_dynamic.summary())

    # â”€â”€ 5B: Static Traffic Simulation (Baseline) â”€â”€
    print("\n[Simulation] Running STATIC traffic simulation (baseline)...")
    sim_static = RobotaxiSimulationModule(
        num_zones=city.num_zones,
        zone_distances=zone_distances,
        seed=config.RANDOM_SEED,
    )

    for step in range(sim_steps):
        step_time = step * config.TIME_STEP_MINUTES
        data_idx = last_day_start + step
        
        actual_demand = {}
        actual_congestion = {}

        for zid in range(city.num_zones):
            data = zone_data[zid]
            actual_demand[zid] = data[data_idx, 0]
            actual_congestion[zid] = data[data_idx, 1]

        sim_static.simulate_step(
            step_time, actual_demand=actual_demand,
            pred_demand={}, actual_congestion=actual_congestion,
            use_dynamic=False
        )

    static_metrics = sim_static.get_overall_metrics()
    print(sim_static.summary())

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 6: Performance Evaluation & Visualization
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("STEP 6: Performance Evaluation Module")
    print("â”€" * 60)

    evaluator = PerformanceEvaluationModule()

    print("\n[Evaluation] Generating visualizations...")

    # 1. Training loss plot
    evaluator.plot_training_loss(round_losses, zone_losses)

    # 2. Prediction accuracy plot
    evaluator.plot_prediction_accuracy(actuals, preds)

    # 3. Compression analysis plot
    savings_per_round = [
        compressor.get_communication_savings()
    ] * config.NUM_FL_ROUNDS
    evaluator.plot_compression_analysis(
        compression_ratios=[config.COMPRESSION_RATIO] * config.NUM_FL_ROUNDS,
        communication_savings=savings_per_round,
        accuracy_with_compression=round_rmses,
        accuracy_without=round_rmses_no_compress,
    )

    # 4. Correlation selection plot
    evaluator.plot_correlation_selection(correlator.selection_history)

    # 5. Simulation comparison plot
    evaluator.plot_simulation_comparison(dynamic_metrics, static_metrics)

    # 6. Traffic evolution plot
    evaluator.plot_traffic_evolution(zone_data, predictor.predictions_history)

    # 7. City demand heatmap
    avg_demands = np.array([
        zone_data[zid][:, 0].mean() for zid in range(city.num_zones)
    ])
    evaluator.plot_city_heatmap(
        avg_demands, config.CITY_GRID_SIZE,
        title='Average Ride Demand Across City Zones',
        filename='demand_heatmap.png'
    )

    # 8. Congestion heatmap
    avg_congestion = np.array([
        zone_data[zid][:, 1].mean() for zid in range(city.num_zones)
    ])
    evaluator.plot_city_heatmap(
        avg_congestion, config.CITY_GRID_SIZE,
        title='Average Congestion Level Across City Zones',
        filename='congestion_heatmap.png'
    )

    # 9. Fleet utilization timeline
    evaluator.plot_fleet_utilization_over_time(sim_dynamic.step_metrics)

    # 10. Generate text report
    fl_metrics = {
        "total_rounds": config.NUM_FL_ROUNDS,
        "final_loss": round_losses[-1],
        "best_loss": min(round_losses),
        "mae": overall_mae,
        "rmse": overall_rmse,
        "mape": overall_mape,
    }
    compression_stats = {
        "target_ratio": f"{config.COMPRESSION_RATIO:.0%}",
        "actual_ratio": f"{compressor.get_compression_ratio_actual():.1%}",
        "savings": f"{compressor.get_communication_savings():.1f}%",
    }
    correlation_stats = {
        "selection_rate": f"{correlator.get_selection_rate():.1%}",
        "total_removed": sum(
            h.get("redundant_removed", 0)
            for h in correlator.selection_history
        ),
    }

    print("\n[Evaluation] Generating report...")
    report = evaluator.generate_report(
        fl_metrics, dynamic_metrics, static_metrics,
        compression_stats, correlation_stats
    )
    print("\n" + report)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Summary
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    elapsed = time.time() - start_time
    print(f"\n{'=' * 70}")
    print(f"  âœ… Pipeline completed in {elapsed:.1f} seconds")
    print(f"  ğŸ“ Results saved to: {config.RESULTS_DIR}")
    print(f"{'=' * 70}")


if __name__ == "__main__":
    main()
